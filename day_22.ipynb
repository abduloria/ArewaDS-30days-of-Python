{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMAbdvBj80Ik+oY0B3fkEEP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\n","!pip install wikipedia-api"],"metadata":{"id":"egc33GUs_OYx"},"execution_count":null,"outputs":[]},{"source":["import requests\n","from bs4 import BeautifulSoup\n","url = 'https://archive.ics.uci.edu/ml/datasets.php'\n","\n","response = requests.get(url)\n","content = response.content # we get all the content from the website\n","soup = BeautifulSoup(content, 'html.parser') # beautiful soup will give a chance to parse\n","print(soup.title) # <title>UCI Machine Learning Repository: Data Sets</title>\n","print(soup.title.get_text()) # UCI Machine Learning Repository: Data Sets\n","print(soup.body) # gives the whole page on the website\n","print(response.status_code)\n","\n","tables = soup.find_all('table', {'cellpadding':'3'})\n","# We are targeting the table with cellpadding attribute with the value of 3\n","# We can select using id, class or HTML tag , for more information check the beautifulsoup doc\n","if tables:\n","    table = tables[0] # the result is a list, we are taking out data from it\n","    for td in table.find('tr').find_all('td'):\n","        print(td.text)\n","# Extract the table data and convert it to JSON\n","table = soup.find('table')\n","data = []\n","for row in table.find_all('tr')[1:]:\n","    columns = row.find_all('td')\n","    data.append({\n","        'Category': columns[0].text.strip(),\n","        'Value': columns[1].text.strip()\n","    })\n","\n","with open('bu_data.json', 'w') as json_file:\n","    json.dump(data, json_file, indent=4)"],"cell_type":"code","metadata":{"id":"tcVBWrKxBZYi"},"execution_count":null,"outputs":[]},{"source":["import requests\n","from bs4 import BeautifulSoup\n","url = 'https://archive.ics.uci.edu/ml/datasets.php'\n","\n","response = requests.get(url)\n","content = response.content # we get all the content from the website\n","soup = BeautifulSoup(content, 'html.parser') # beautiful soup will give a chance to parse\n","print(soup.title) # <title>UCI Machine Learning Repository: Data Sets</title>\n","print(soup.title.get_text()) # UCI Machine Learning Repository: Data Sets\n","print(soup.body) # gives the whole page on the website\n","print(response.status_code)\n","\n","tables = soup.find_all('table', {'cellpadding':'3'})\n","# We are targeting the table with cellpadding attribute with the value of 3\n","# We can select using id, class or HTML tag , for more information check the beautifulsoup doc\n","if not tables:\n","    print('No tables found.')\n","else:\n","    table = tables[0] # the result is a list, we are taking out data from it\n","    for td in table.find('tr').find_all('td'):\n","        print(td.text)\n","# Extract the table data and convert it to JSON\n","table = soup.find('table')\n","data = []\n","for row in table.find_all('tr')[1:]:\n","    columns = row.find_all('td')\n","    data.append({\n","        'Category': columns[0].text.strip(),\n","        'Value': columns[1].text.strip()\n","    })\n","\n","with open('bu_data.json', 'w') as json_file:\n","    json.dump(data, json_file, indent=4)"],"cell_type":"code","metadata":{"id":"ewqYcZ5DK54T"},"execution_count":null,"outputs":[]},{"source":["import requests\n","from bs4 import BeautifulSoup\n","url = 'https://archive.ics.uci.edu/ml/datasets.php'\n","\n","response = requests.get(url)\n","content = response.content # we get all the content from the website\n","soup = BeautifulSoup(content, 'html.parser') # beautiful soup will give a chance to parse\n","print(soup.title) # <title>UCI Machine Learning Repository: Data Sets</title>\n","print(soup.title.get_text()) # UCI Machine Learning Repository: Data Sets\n","print(soup.body) # gives the whole page on the website\n","print(response.status_code)\n","\n","tables = soup.find_all('table', {'cellpadding':'3'})\n","# We are targeting the table with cellpadding attribute with the value of 3\n","# We can select using id, class or HTML tag , for more information check the beautifulsoup doc\n","if not tables:\n","    print('No tables found.')\n","else:\n","    table = tables[0] # the result is a list, we are taking out data from it\n","    for td in table.find('tr').find_all('td'):\n","        print(td.text)\n","# Extract the table data and convert it to JSON\n","if table is None:\n","    table = BeautifulSoup(response.content, 'html.parser')\n","data = []\n","for row in table.find_all('tr')[1:]:\n","    columns = row.find_all('td')\n","    data.append({\n","        'Category': columns[0].text.strip(),\n","        'Value': columns[1].text.strip()\n","    })\n","\n","with open('bu_data.json', 'w') as json_file:\n","    json.dump(data, json_file, indent=4)"],"cell_type":"code","metadata":{"id":"yCHkzteALDBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","url_uci = 'https://archive.ics.uci.edu/ml/datasets.php'\n","\n","response_uci = requests.get(url_uci)\n","soup_uci = BeautifulSoup(response_uci.text, 'html.parser')\n","\n","# Extract the table data and convert it to JSON\n","table_uci = soup_uci.find('table')\n","data_uci = []\n","for row_uci in table_uci.find_all('tr')[1:]:\n","    columns_uci = row_uci.find_all('td')\n","    data_uci.append({\n","        'Dataset Name': columns_uci[0].text.strip(),\n","        'Default Task': columns_uci[1].text.strip()\n","        # Add more columns as needed\n","    })\n","\n","with open('uci_data.json', 'w') as json_file_uci:\n","    json.dump(data_uci, json_file_uci, indent=4)"],"metadata":{"id":"nrAG1XOc8BvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import wikipediaapi\n","import json\n","\n","wiki_wiki = wikipediaapi.Wikipedia('en')\n","page_py = wiki_wiki.page(\"List_of_presidents_of_the_United_States\")\n","presidents_data = []\n","\n","# Parse the presidents table\n","for section in page_py.sections:\n","    if section.title == \"List of presidents\":\n","        presidents_table = section.text\n","        # Parse the table data and add it to the presidents_data list\n","\n","# Save the data as JSON\n","with open('presidents_data.json', 'w') as json_file_presidents:\n","    json.dump(presidents_data, json_file_presidents, indent=4)"],"metadata":{"id":"JqE-XC-k8LiB"},"execution_count":null,"outputs":[]}]}